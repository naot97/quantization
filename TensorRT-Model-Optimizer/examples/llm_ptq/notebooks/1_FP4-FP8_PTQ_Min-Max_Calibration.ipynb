{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9161daa2-03a6-41cd-b349-410004ab37c4",
   "metadata": {},
   "source": [
    "# Post-Training Quantization with Min-Max Calibration using TensorRT Model Optimizer PTQ\n",
    "\n",
    "This notebook demonstrates how to apply standard Post-Training Quantization (PTQ) using min-max calibration on an LLM‚Äîspecifically meta-llama/Llama-3.1-8B-Instruct‚Äîwith NVIDIA's TensorRT Model Optimizer (ModelOpt) PTQ toolkit. We walk through loading the model, calibrating it using a CNN/DailyMail dataset sample, applying FP8 quantization, generating outputs, and exporting the quantized model.\n",
    "\n",
    "Key Dependencies:\n",
    "- nvidia-modelopt\n",
    "- torch\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972a570-2549-4494-b56e-9a901c5c2286",
   "metadata": {},
   "source": [
    "## Standard FP4/FP8 Quantization with Min-Max Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e4d08-e85f-4d2f-b22d-c11a15268c8b",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies\n",
    "Import all necessary libraries:\n",
    "\n",
    "- `torch`: Used for tensor computation and model execution.\n",
    "\n",
    "- `modelopt.torch.quantization`: Core API for quantization using TensorRT ModelOpt PTQ.\n",
    "\n",
    "- `transformers`: Hugging Face interface to load and tokenize LLMs.\n",
    "\n",
    "- `get_dataset_dataloader` and `create_forward_loop`: Utilities to prepare calibration data and run calibration.\n",
    "\n",
    "- `login`: Required to download gated models (like Llama 3.1) from Hugging Face.\n",
    "\n",
    "üí° If you're using this in Colab or a restricted environment, make sure all packages are installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31ffa95-21cb-49a4-bf96-a0e626016466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "from modelopt.torch.utils.dataset_utils import create_forward_loop, get_dataset_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a654c1-54ad-4597-a1e3-985822aac5ff",
   "metadata": {},
   "source": [
    "### 2. Set Configurations and Login to Hugging Face\n",
    "\n",
    "Set the model you want to quantize (Llama-3.1-8B-Instruct) and the dataset to use for calibration (cnn_dailymail).\n",
    "\n",
    "- `batch_size` and `calib_samples` control how much data is used during calibration‚Äîmore samples improve accuracy but - increase calibration time.\n",
    "\n",
    "üîê You must `login()` with a valid Hugging Face token to access gated models. Get your token at hf.co/settings/tokens.\n",
    "\n",
    "üîÅ You can substitute your own model or dataset as long as the inputs are compatible with the model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a201f-4cb1-43bb-b0a9-c859ea6eb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "batch_size = 8\n",
    "calib_samples = 512\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b233aae-5341-4f9b-9183-3ccf94af0e89",
   "metadata": {},
   "source": [
    "### 3. Load Model and Tokenizer\n",
    "\n",
    "- Load the model into GPU memory.\n",
    "- Set `pad_token` to eos_token to prevent padding errors in decoder-only models like Llama.\n",
    "\n",
    "üí° Always check for token mismatch warnings in console when loading tokenizer.\n",
    "üß† Setting `pad_token` helps avoid errors during batch generation or dataset collation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6b892-dc12-4549-8841-13016da409c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb43104-4251-4634-9efc-312f9a5f0dc6",
   "metadata": {},
   "source": [
    "### 4. Configure Dataloader\n",
    "- Load a few batches of real-world text to extract representative activation ranges.\n",
    "- The calibration dataset should reflect your expected inference use case for best results.\n",
    "\n",
    "‚ö†Ô∏è More samples = better accuracy, but takes longer. We recommend 512 samples or more. \n",
    "üß™ Use your target task‚Äôs dataset (e.g., chat, summarization, code) for domain-specific calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f87c1-9d74-4b2b-9250-3709f81d0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataset_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    num_samples=calib_samples,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f84b1-9557-4a90-b0fe-a539a54f63da",
   "metadata": {},
   "source": [
    "### 5. Create the Forward Loop\n",
    "- Wraps your `dataloader` into a loop that feeds batches into the model.\n",
    "- Required by `modelopt.quantize()` to perform calibration pass.\n",
    "\n",
    "üß∞ You can create your own custom forward loop if you're doing multi-modal or conditional generation tasks.\n",
    "üß† ModelOpt expects this loop to return outputs so it can record activations for min/max stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b774f65d-a6b5-409e-afc0-9d09589c9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_loop = create_forward_loop(dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14930c-baa9-4ae8-9b2a-0a412949b204",
   "metadata": {},
   "source": [
    "### 6. Set Quantization Configuration and Apply\n",
    "- Apply FP8 quantization using the default min-max config provided by TensorRT ModelOpt.\n",
    "- This pass captures the range of activations and applies a quantization transform.\n",
    "- To change the quantization configuration, you simply need to change the value of the `quant_cfg` variable. For example, to change this from FP8 to NVFP4, you can set it to `mtq.NVFP4_DEFAULT_CFG`\n",
    "\n",
    "üìè Min-max calibration uses observed min and max values per tensor to set scaling ranges.\n",
    "üí° You can experiment with other formats (e.g., FP4, INT8) by swapping out quant_cfg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce3b47-48ac-4a27-a5ed-351a10c104a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cfg = mtq.FP8_DEFAULT_CFG  # mtq.NVFP4_DEFAULT_CFG\n",
    "model = mtq.quantize(model, quant_cfg, forward_loop=forward_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ffc8f-7f04-4017-af53-54f4849646c5",
   "metadata": {},
   "source": [
    "### 7. Quick Test of Quantized Model\n",
    "- Test the quantized model with a simple prompt.\n",
    "- This helps verify that quantization didn‚Äôt break forward generation or drastically harm output quality.\n",
    "\n",
    "‚úÖ Expect slightly more variation or truncation in output compared to the original model, but it should still be coherent.\n",
    "üß™ You can test on more complex prompts to evaluate qualitative performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a601bfc-0637-409d-bbf7-b97a2bbf6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77452d-67e2-41e0-818e-6cacdd9c3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dc86d-9d09-45cc-96df-b9ddcebaf917",
   "metadata": {},
   "source": [
    "### 8. Export Quantized Checkpoint\n",
    "- Save the quantized model in Hugging Face-compatible format for reuse or deployment.\n",
    "- Export includes weights and config files in standard structure.\n",
    "\n",
    "üìÅ This allows you to upload it to Hugging Face Hub or load later with from_pretrained() üß∞ You can also use this exported model with inference engines like vLLM, SGLang, or TensorRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa6720-6cc9-4668-a325-263035b17e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "export_path = \"./quantized_model_min-max/\"\n",
    "export_hf_checkpoint(model, export_dir=export_path)\n",
    "tokenizer.save_pretrained(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934d62d-3b38-4efa-95a9-e5b27634a365",
   "metadata": {},
   "source": [
    "# ‚úÖ Conclusion & Key Takeaways\n",
    "    ‚úÖ Min-max calibration is a fast and simple way to apply quantization with good performance tradeoffs.\n",
    "\n",
    "    ‚úÖ TensorRT-LLM ModelOpt PTQ abstracts away many of the complexities of quantization while still offering flexibility and export options.\n",
    "\n",
    "    ‚úÖ Using a representative dataset like cnn_dailymail improves calibration accuracy for summarization-style models.\n",
    "\n",
    "    ‚úÖ The quantized model remains Hugging Face-compatible‚Äîmeaning it can be deployed or fine-tuned using existing tools.\n",
    "\n",
    "    ‚úÖ You can easily customize: The quantization format (e.g., INT8, FP4), Calibration samples and batch size, and Dataset/task alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7aee5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
