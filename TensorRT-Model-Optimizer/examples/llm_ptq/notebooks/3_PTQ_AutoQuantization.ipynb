{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80da483-f81a-408f-9755-6c1bc9d84119",
   "metadata": {},
   "source": [
    "# AutoQuantization with TensorRT Model Optimizer PTQ\n",
    "\n",
    "This notebook demonstrates how to use ModelOpt PTQ's auto_quantize feature to perform automated mixed-precision quantization on the Meta-LLaMA-3-8B model. You'll define a target effective bit rate (e.g., 8.0), provide a search space of quantization formats, and optionally include KV cache quantization.\n",
    "\n",
    "The process automatically searches the quantization format and layer mapping that best satisfies the target bit constraint while minimizing accuracy loss‚Äîusing loss-based scoring and real calibration data.\n",
    "\n",
    "Key Dependencies: \n",
    "- nvidia-modelopt\n",
    "- torch\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae88b2-b51b-40fc-8ef3-4ff0bf37916b",
   "metadata": {},
   "source": [
    "# Applying AutoQuantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247af06-7752-4d28-8fa1-22af961130a3",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies\n",
    "\n",
    "Load general-purpose and reproducibility packages. Random seeds will be set for deterministic calibration, and the Hugging Face login is required to pull the model.\n",
    "\n",
    "- Import core ModelOpt utilities for quantization, calibration, and dataset handling.\n",
    "- init_quantized_weights is used internally by some ModelOpt features for weight initialization‚Äîno need to call directly here, but it supports hybrid quantized weight loading when needed.\n",
    "- Set fixed seeds for reproducibility. This ensures consistent calibration data selection and loss values during scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942eb87c-3e69-49dd-a7b6-933e5a700b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "from modelopt.torch.utils.dataset_utils import (\n",
    "    create_forward_loop,\n",
    "    get_dataset_dataloader,\n",
    "    get_max_batch_size,\n",
    ")\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f71edaf-e3b5-469a-a36c-b1b93c033c60",
   "metadata": {},
   "source": [
    "### 2. Set Configurations and Login to Hugging Face\n",
    "\n",
    "Define all major tuning knobs:\n",
    "\n",
    "- `EFFECTIVE_BITS` is the average precision target across quantized layers.\n",
    "- `Q_FORMATS` defines the list of quantization formats used during the AutoQuant search.\n",
    "- `KV_FORMAT` allows optional quantization of KV cache, applied after main quant.\n",
    "- `EXPORT_FMT` supports exporting to Hugging Face (`hf`) or TensorRT-LLM (`tensorrt_llm`).\n",
    "- Also logs into Hugging Face\n",
    "\n",
    "üí° Try adding formats like `\"nvfp4\"`, `\"w4a8_awq\"` to explore tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5814162-f577-4c05-8573-36aad32323c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATASET = \"cnn_dailymail\"\n",
    "CALIB_SAMPLES = 512\n",
    "EFFECTIVE_BITS = 6.0  # search target\n",
    "Q_FORMATS = \"fp8,int4_awq\"  # search space\n",
    "KV_FORMAT = \"none\"  # or \"none\" to skip\n",
    "EXPORT_DIR = \"llama3_8b_autoq\"  # output folder\n",
    "EXPORT_FMT = \"tensorrt_llm\"  # or \"hf\"\n",
    "# ----------------------------\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = torch.float16  # keep default for faster search\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a92e4-7bf1-485b-a283-3c69b64d2767",
   "metadata": {},
   "source": [
    "### 3. Load Model and Tokenizer\n",
    "\n",
    "Load model and tokenizer into memory:\n",
    "\n",
    "- torch_dtype=torch.float16 is used to reduce memory during calibration.\n",
    "- Left padding is preferred for decoder-only LLMs to better align prompt-token positions during calibration.\n",
    "\n",
    "‚ö†Ô∏è Ensure the pad_token is set for batching; some LLaMA variants may not have it by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c91ffc-c73d-44df-b833-e773dca0457e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=DTYPE).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259ff10-3d05-44fa-b650-bcdbcadc74ad",
   "metadata": {},
   "source": [
    "### 4. Configure Data Loader and Forward Loop\n",
    "\n",
    "- Use a small number of real samples to capture representative activations and enable loss-based scoring.\n",
    "- include_labels=True is important for loss computation, which guides AutoQuant format decisions.\n",
    "\n",
    "‚öôÔ∏è get_max_batch_size() estimates the largest batch that fits in memory given model size and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e25a0-f271-4ca4-ae53-f3f77a09fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = min(get_max_batch_size(model), CALIB_SAMPLES)\n",
    "calib_loader = get_dataset_dataloader(\n",
    "    dataset_name=DATASET,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    num_samples=CALIB_SAMPLES,\n",
    "    device=DEVICE,\n",
    "    include_labels=True,\n",
    ")\n",
    "forward_loop = create_forward_loop(dataloader=calib_loader)\n",
    "print(f\"Calibration batches: {len(calib_loader)}  |  Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056b7c2-aee1-4d63-b062-42430750104f",
   "metadata": {},
   "source": [
    "### 5. Possible Quantization Configurations\n",
    "\n",
    "Define lookup tables for available quantization config presets. These are used to construct the format search space for AutoQuant.\n",
    "\n",
    "‚úÖ You can freely extend these dictionaries to add custom formats or constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c135ff-6acb-4a0e-a447-0774d9d269cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANT_CFG = {\n",
    "    \"int8\": mtq.INT8_DEFAULT_CFG,\n",
    "    \"int8_sq\": mtq.INT8_SMOOTHQUANT_CFG,\n",
    "    \"fp8\": mtq.FP8_DEFAULT_CFG,\n",
    "    \"int4_awq\": mtq.INT4_AWQ_CFG,\n",
    "    \"nvfp4\": mtq.NVFP4_DEFAULT_CFG,\n",
    "    \"nvfp4_awq\": mtq.NVFP4_AWQ_LITE_CFG,\n",
    "    \"w4a8_awq\": mtq.W4A8_AWQ_BETA_CFG,\n",
    "}\n",
    "\n",
    "KV_CFG = {\n",
    "    \"none\": None,\n",
    "    \"fp8\": mtq.FP8_KV_CFG[\"quant_cfg\"],\n",
    "    \"nvfp4\": mtq.NVFP4_KV_CFG[\"quant_cfg\"],\n",
    "    \"nvfp4_affine\": mtq.NVFP4_AFFINE_KV_CFG[\"quant_cfg\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c58b3-816d-47e9-89b4-65dc7362f22f",
   "metadata": {},
   "source": [
    "### 6. Start AutoQuantization Search and Optimization\n",
    "\n",
    "Wrap the model‚Äôs native loss function. Required for format scoring during AutoQuant‚Äîsmaller loss = better format match.\n",
    "\n",
    "Automatically search the best per-layer quantization format mapping:\n",
    "\n",
    "- Constraints guide the average bit precision.\n",
    "- Loss is evaluated across candidate formats to preserve accuracy.\n",
    "- disabled_layers=[\"*lm_head*\"] keeps the final layer unquantized (important for generation quality).\n",
    "\n",
    "üîç Verbose mode shows layer-level decisions and scoring for each candidate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cf632-25e3-4ab0-a217-22843ea15138",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(out, batch):  # tiny wrapper around HF loss\n",
    "    return out.loss\n",
    "\n",
    "\n",
    "print(\"üöß  Launching auto_quantize ...\")\n",
    "t0 = time.time()\n",
    "\n",
    "model, _ = mtq.auto_quantize(\n",
    "    model,\n",
    "    constraints={\"effective_bits\": EFFECTIVE_BITS},\n",
    "    data_loader=calib_loader,\n",
    "    forward_step=lambda m, b: m(**b),\n",
    "    loss_func=loss_fn,\n",
    "    quantization_formats=[QUANT_CFG[q] for q in Q_FORMATS.split(\",\")],\n",
    "    num_calib_steps=len(calib_loader),\n",
    "    num_score_steps=len(calib_loader),\n",
    "    verbose=True,\n",
    "    disabled_layers=[\"*lm_head*\"],  # keep LM head in fp16\n",
    ")\n",
    "print(f\"‚úÖ Done in {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba5f17-932f-41c8-a27f-c6b812a041a5",
   "metadata": {},
   "source": [
    "### 7. [Optional] KV Cache AutoQuantization\n",
    "\n",
    "- This happens after main quantization.\n",
    "- Only KV-specific quantizers are enabled during this pass.\n",
    "\n",
    "‚ö†Ô∏è Quantizing KV cache may affect generation performance and context retention‚Äîtest thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b805b1-8583-4fdb-afeb-b266821ebe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KV_FORMAT != \"none\":\n",
    "    print(f\"Enabling KV cache quantization ‚ü∂ {KV_FORMAT}\")\n",
    "    kv_cfg = KV_CFG[KV_FORMAT]\n",
    "\n",
    "    # Plug only the KV quantizers\n",
    "    mtq.set_quantizer_by_cfg(model, quant_cfg=kv_cfg)\n",
    "\n",
    "    # Calibrate **only** those quantizers\n",
    "    with mtq.set_quantizer_by_cfg_context(model, {\"*\": {\"enable\": False}, **kv_cfg}):\n",
    "        mtq.calibrate(model, algorithm=\"max\", forward_loop=forward_loop)\n",
    "else:\n",
    "    print(\"KV cache left unquantized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6f061-a1d8-45c1-a390-aa5730d5a60d",
   "metadata": {},
   "source": [
    "### 8. Inspect the Quantized Layers\n",
    "\n",
    "Print a full summary of quantized layers, formats, and bit precision estimates‚Äîuseful for debugging or profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20343e9-2ae0-4434-a19d-d7b6e912ba74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtq.print_quant_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ad1e8-f200-4efa-aa23-dd7eec52516c",
   "metadata": {},
   "source": [
    "### 9. Quick Test of Quantized Model\n",
    "\n",
    "Sanity check: Run a quick generation to verify the quantized model produces reasonable output.\n",
    "\n",
    "üß™ Consider adding prompts from your real use case to validate quality before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992c873-02ec-4886-a47d-d38f8e4b405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Tell me a short story about a quantized llama.\"\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    gen_ids = model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7598596-9eca-42a1-bf7f-665a8023c48c",
   "metadata": {},
   "source": [
    "### 10. Export Model for TensorRT-LLM\n",
    "\n",
    "Export the quantized model to the desired format:\n",
    "\n",
    "- Use tensorrt_llm for high-performance deployment on NVIDIA accelerators.\n",
    "- Use hf to reload with Hugging Face APIs or inference frameworks like vLLM.\n",
    "\n",
    "üìÅ Check the contents of the output folder to confirm all weights/configs are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed5ad6-fd9f-44dd-b2c0-aefc46d1d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_hf_checkpoint, export_tensorrt_llm_checkpoint\n",
    "\n",
    "if EXPORT_FMT == \"tensorrt_llm\":\n",
    "    export_tensorrt_llm_checkpoint(\n",
    "        model,\n",
    "        model_type=\"llama\",\n",
    "        export_dir=EXPORT_DIR,\n",
    "        inference_tensor_parallel=1,\n",
    "        inference_pipeline_parallel=1,\n",
    "    )\n",
    "else:\n",
    "    export_hf_checkpoint(model, export_dir=EXPORT_DIR)\n",
    "\n",
    "print(f\"üì¶  Saved quantized model to ‚Üí  {EXPORT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638588fa-3781-4ac4-a77e-140331caba80",
   "metadata": {},
   "source": [
    "# ‚úÖ Conclusion & Key Takeaways\n",
    "    ‚úÖ AutoQuant in TensorRT-LLM ModelOpt enables fast, automated mixed-precision quantization by searching across multiple formats (e.g., FP8, INT4-AWQ) to meet a user-defined effective bit constraint.\n",
    "\n",
    "    ‚úÖ Using a small calibration set with loss-based scoring, AutoQuant intelligently selects the optimal quantization format per layer‚Äîbalancing model size, performance, and accuracy.\n",
    "\n",
    "    ‚úÖ The workflow supports flexible search spaces and fine-grained control over disabled layers, block sizes, and forward calibration passes.\n",
    "\n",
    "    ‚úÖ Optional KV cache quantization provides further memory and bandwidth savings, but should be enabled only after validating generation quality.\n",
    "\n",
    "    ‚úÖ Exported models are fully compatible with both Hugging Face and TensorRT-LLM inference runtimes‚Äîenabling rapid deployment across a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c029c-5f9c-4f0c-8db7-db7e9d8800ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
